# ğŸš€ End-to-End Text Summarization System (Production-Ready)

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red.svg)](https://pytorch.org/)
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg)](https://huggingface.co/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Production-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Containerized-blue.svg)](https://www.docker.com/)
[![AWS](https://img.shields.io/badge/AWS-EC2%20%7C%20ECR-orange.svg)](https://aws.amazon.com/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-success.svg)](https://github.com/features/actions)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A **production-grade NLP system** for abstractive text summarization built using **Hugging Face Transformers**, **PyTorch**, and **modern MLOps practices**.

This project implements an end-to-end text summarization pipeline using Natural Language Processing (NLP) techniques. The model is deployed using AWS EC2 and AWS ECR with CI/CD automation through GitHub Actions.

This project demonstrates how to design, train, deploy, and scale a real-world machine learning system with CI/CD automation and cloud deployment.


## ğŸ” Project Overview

This repository implements an **end-to-end text summarization pipeline** that covers:

- Data ingestion and validation  
- Text preprocessing and transformation  
- Model training and evaluation  
- API-based inference  
- Containerization and cloud deployment  

The system is designed with **modularity, scalability, and production-readiness** in mind.


## ğŸ§  Key Features

- ğŸ”¹ End-to-end ML pipeline (ingestion â†’ inference)
- ğŸ”¹ Modular, clean architecture
- ğŸ”¹ Hugging Face PEGASUS-based abstractive summarization
- ğŸ”¹ Configuration-driven design (YAML-based)
- ğŸ”¹ CI/CD with GitHub Actions
- ğŸ”¹ Dockerized & deployed on AWS EC2
- ğŸ”¹ Logging, validation, and monitoring support


## ğŸ—ï¸ Project Structure
```text
End-to-End-Text-Summarization-Project/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ main.yaml                     # CI/CD pipeline (GitHub Actions)
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                      # Project configuration file
â”‚
â”œâ”€â”€ research/
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb               
â”‚   â”œâ”€â”€ 02_data_validation.ipynb              
â”‚   â”œâ”€â”€ 03_data_transformation.ipynb          
â”‚   â”œâ”€â”€ 04_model_trainer.ipynb
â”‚   â”œâ”€â”€ 05_model_evaluation.ipynb
â”‚   â”œâ”€â”€ text_summarization.ipynb             
â”‚   â””â”€â”€ trials.ipynb
â”‚
â”œâ”€â”€ src/
    â””â”€â”€ textSummarizer
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ components/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ data_ingestion.py               # Dataset download & extraction
        â”‚   â”œâ”€â”€ data_validation.py              # Dataset validation logic
        â”‚   â”œâ”€â”€ data_transformation.py          # Tokenization & preprocessing
        â”‚   â”œâ”€â”€ model_trainer.py                # Model training logic
        â”‚   â””â”€â”€ model_evaluation.py             # Evaluation and metrics
        â”‚
        â”œâ”€â”€ config/
        â”‚   â”œâ”€â”€ __init__.py                      
        â”‚   â””â”€â”€ configuration.py                      
        â”‚
        â”œâ”€â”€ constants/
        â”‚   â””â”€â”€ __init__.py                      # Global constants
        â”‚
        â”œâ”€â”€ entity/
        â”‚   â””â”€â”€ __init__.py                      # Dataclass-based configuration schemas
        â”‚
        â”œâ”€â”€ logging/
        â”‚   â””â”€â”€ __init__.py                      
        â”‚
        â”œâ”€â”€ pipeline/
        â”‚   â”œâ”€â”€ __init__.py      
        â”‚   â”œâ”€â”€ prediction.py     
        â”‚   â”œâ”€â”€ stage_01_data_ingestion.py      # Data ingestion pipeline
        â”‚   â”œâ”€â”€ stage_02_data_validation.py     # Data validation pipeline
        â”‚   â”œâ”€â”€ stage_03_data_transformation.py # Data transformation pipeline
        â”‚   â”œâ”€â”€ stage_04_model_trainer.py       # Model training pipeline
        â”‚   â””â”€â”€ stage_05_model_evaluation.py    # Model evaluation pipeline
        â”‚
        â””â”€â”€ utils/
            â”œâ”€â”€ __init__.py
            â””â”€â”€ common.py                        # Utility functions (I/O, helpers)
â”‚
â”œâ”€â”€ .gitignore                          # Git ignore rules
â”œâ”€â”€ app.py                              # FastAPI inference service
â”œâ”€â”€ Dockerfile                          # Container configuration
â”œâ”€â”€ LICENSE                             # license
â”œâ”€â”€ main.py                             # Orchestrates full pipeline execution
â”œâ”€â”€ params.yaml                         # Training & hyperparameter config
â”œâ”€â”€ README.md                           # Project documentation
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ setup.py                            # Package setup
â””â”€â”€ template.py
```


## âš™ï¸ Tech Stack

| Category | Tools |
|------|------|
| Language | Python |
| NLP | Hugging Face Transformers (PEGASUS) |
| Training | PyTorch |
| Evaluation | ROUGE |
| API | FastAPI |
| MLOps | Docker, GitHub Actions |
| Cloud | AWS EC2 & ECR |
| Configuration | YAML |
| Logging | Python Logging |


## ğŸ”„ Pipeline Workflow

1. **Data Ingestion** â€“ Downloads and extracts dataset  
2. **Data Validation** â€“ Ensures data integrity  
3. **Data Transformation** â€“ Tokenization & preprocessing  
4. **Model Training** â€“ Fine-tuning PEGASUS  
5. **Model Evaluation** â€“ ROUGE metrics  
6. **Inference** â€“ Real-time summarization via REST API  


## ğŸš€ Running Locally

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/shaadclt/End-to-End-Text-Summarization-Project.git
cd End-to-End-Text-Summarization-Project
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
conda create -n summarizer python=3.9 -y
conda activate summarizer
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Application
```bash
uvicorn app:app --host 0.0.0.0 --port 8080
```

Access the API at:
ğŸ‘‰ `http://localhost:8080`

## ğŸ³ Docker Deployment
```bash
docker build -t text-summarizer .
docker run -p 8080:8080 text-summarizer
```

## â˜ï¸ Cloud Deployment (AWS)

- Dockerized and pushed to Amazon ECR
- Deployed on EC2
- CI/CD via GitHub Actions
- Automated build, test, and deploy pipeline

## ğŸ‘¤ Author

**Mohamed Shaad**

Machine Learning Engineer


